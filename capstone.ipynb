{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jxKyUPyKAg-",
        "outputId": "063b9c48-a802-4341-c8f6-ed856e078387"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EisClCzUH5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307f6518-a957-4329-e8de-fc8a11b70738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CapstoneDataset/training/01_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/02_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/03_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/04_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/05_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/06_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/07_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/08_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/09_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/10_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/11_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/12_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/13_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/14_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/15_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/16_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/17_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/18_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/01_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/02_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/03_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/04_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/05_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/06_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/07_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/08_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/09_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/10_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/11_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/12_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/13_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/14_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/15_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/16_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/17_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/18_test.csv\n"
          ]
        }
      ],
      "source": [
        "# Read dataset from csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/CapstoneDataset/training'\n",
        "test_dir = '/content/drive/MyDrive/CapstoneDataset/test'\n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "file_number = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18']\n",
        "\n",
        "for number in file_number:\n",
        "  print(train_dir+'/'+number+'_train.csv')\n",
        "  df = pd.read_csv(train_dir+'/'+number+'_train.csv',header=5)\n",
        "  df = df.drop('Frame', axis=1)\n",
        "  df = df.drop('Time (Seconds)', axis=1)\n",
        "  train_data.append(df)\n",
        "\n",
        "for number in file_number:\n",
        "  print(test_dir+'/'+number+'_test.csv')\n",
        "  df = pd.read_csv(test_dir+'/'+number+'_test.csv',header=5)\n",
        "  df = df.drop('Frame', axis=1)\n",
        "  df = df.drop('Time (Seconds)', axis=1)\n",
        "  test_data.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part took me over one hour. I exported the final result to csv file. I won't run this part again."
      ],
      "metadata": {
        "id": "9h1hsHcRoQxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Furthest Point Sampling\n",
        "from os import remove\n",
        "def read_points(x,y,z):\n",
        "    points = np.stack([x, y, z], axis=1)\n",
        "    return points\n",
        "\n",
        "''' Strategy of FPS\n",
        "Step 1: Use the first point in the point cloud as the query point and take the furthest point from the remaining points\n",
        "Step 2: Continue with the point taken out as the query point and take the furthest point from the remaining points. \n",
        "Now, since the number of points already taken out is greater than 1, each point in the set of points already selected needs to be taken into account, \n",
        "The calculation logic is as follows.\n",
        "    Step 2.1: For any of the remaining points, calculate the distance from that point to all points in the selected point set.\n",
        "    Step 2.2: Take the minimum value, which is the distance from the remaining points to the selected set of points.\n",
        "    Step 2.3: After calculating the distance of each remaining point to the point set, take the point with the maxiumn distance.\n",
        "Step 3: repeat step 2 until the number K=100.\n",
        "'''\n",
        "class FPS:\n",
        "    # eliminate duplicate points\n",
        "    def __init__(self, points):\n",
        "        print('init start')\n",
        "        self.points = np.unique(points, axis=0)\n",
        "        print('init finish')\n",
        "\n",
        "    # calculate the distance from any of the remaining points to all points in the selected point set. np.stack()\n",
        "    # Take the minumum distance as the the distance from the remaining points to the selected set of points. np.min()\n",
        "    # After calculating the distance of each remaining point to the point set, take the point with the maxiumn distance. np.argmax()\n",
        "    def get_min_distance(self, a, b):\n",
        "        distance = []\n",
        "        for i in range(a.shape[0]):\n",
        "            dis = np.sum(np.square(a[i] - b), axis=-1)\n",
        "            distance.append(dis)\n",
        "        distance = np.stack(distance, axis=-1)\n",
        "        distance = np.min(distance, axis=-1)\n",
        "        return np.argmax(distance)\n",
        "    \n",
        "    # find minimun xyz value and maximum xyz valueã€‚\n",
        "    # Arrange and combine all values to obtain the cornermost point set in the point cloud.\n",
        "    @staticmethod\n",
        "    def get_model_corners(model):\n",
        "        min_x, max_x = np.min(model[:, 0]), np.max(model[:, 0])\n",
        "        min_y, max_y = np.min(model[:, 1]), np.max(model[:, 1])\n",
        "        min_z, max_z = np.min(model[:, 2]), np.max(model[:, 2])\n",
        "        corners_3d = np.array([\n",
        "            [min_x, min_y, min_z],\n",
        "            [min_x, min_y, max_z],\n",
        "            [min_x, max_y, min_z],\n",
        "            [min_x, max_y, max_z],\n",
        "            [max_x, min_y, min_z],\n",
        "            [max_x, min_y, max_z],\n",
        "            [max_x, max_y, min_z],\n",
        "            [max_x, max_y, max_z],\n",
        "        ])\n",
        "        return corners_3d\n",
        "    def compute_fps(self, K):\n",
        "        # compute centre point location according to the cornermost point set\n",
        "        corner_3d = self.get_model_corners(self.points)\n",
        "        center_3d = (np.max(corner_3d, 0) + np.min(corner_3d, 0)) / 2\n",
        "        A = np.array([center_3d])\n",
        "        B = np.array(self.points)\n",
        "        t = []\n",
        "        # looking for k nodes\n",
        "        for i in range(K):\n",
        "            max_id = self.get_min_distance(A, B)\n",
        "            A = np.append(A, np.array([B[max_id]]), 0)\n",
        "            B = np.delete(B, max_id, 0)\n",
        "            t.append(max_id)\n",
        "        return A, t\n",
        "\n",
        "# Calculate the center point of 89 markers. The result is a nrows*3 matrix.\n",
        "def compute_center_point(dataframe):\n",
        "  points = pd.DataFrame(data=None,columns=['X','Y','Z'])\n",
        "  index = 0\n",
        "  size = dataframe.shape[1]\n",
        "  for row in dataframe.index:\n",
        "    x = []\n",
        "    y = []\n",
        "    z = []\n",
        "    for i in range(2,size+1,3):\n",
        "      x.append(dataframe.loc[row][i-2])\n",
        "      y.append(dataframe.loc[row][i-1])\n",
        "      z.append(dataframe.loc[row][i])\n",
        "    points.loc[index] = [sum(x)/len(x),sum(y)/len(y),sum(z)/len(z)]\n",
        "    index = index + 1\n",
        "  return points\n",
        "\n",
        "# Export sampled data to csv\n",
        "sampled_train_dir = '/content/drive/MyDrive/CapstoneDataset/sampled_training'\n",
        "sampled_test_dir = '/content/drive/MyDrive/CapstoneDataset/sampled_test'\n",
        "\n",
        "\n",
        "# First step, calculate the center point of 89 markers in each frame.\n",
        "# Second step, use FPS to find the 100 furthest points and index.\n",
        "# Third step, back to the original dataset to find corresponding 89 markers. 89*100 matrix.\n",
        "count = 1\n",
        "for df in train_data:\n",
        "  points = compute_center_point(df)\n",
        "  print(count)\n",
        "  f = FPS(read_points(points['X'],points['Y'],points['Z']))\n",
        "\n",
        "  # Get the index of the 100 furthest points\n",
        "  # Change the sampling number here\n",
        "  (C,index) = f.compute_fps(100)\n",
        "\n",
        "  # create a new dataframe has the same column as old df\n",
        "  # return to original point cloud and get 89 points according to index. The result is a 100*89 matrix.\n",
        "  data1 = pd.DataFrame(columns = df.columns.to_list())\n",
        "  for i in index:\n",
        "    data1 = data1.append(df.loc[i],ignore_index=True)\n",
        "  data1.to_csv(sampled_train_dir+'/'+str(count)+'.csv', index=False)\n",
        "  print('finish output')\n",
        "  count = int(count) + 1\n",
        "\n",
        "count = 1\n",
        "for df in test_data:\n",
        "  points = compute_center_point(df)\n",
        "  print(count)\n",
        "  f = FPS(read_points(points['X'],points['Y'],points['Z']))\n",
        "\n",
        "  # Change the sampling number here\n",
        "  (C,index) = f.compute_fps(100)\n",
        "\n",
        "  # create a new dataframe has the same column as old df and export it to csv\n",
        "  data1 = pd.DataFrame(columns = df.columns.to_list())\n",
        "  for i in index:\n",
        "    data1 = data1.append(df.loc[i],ignore_index=True)\n",
        "  data1.to_csv(sampled_test_dir+'/'+str(count)+'.csv', index=False)\n",
        "  print('finish output')\n",
        "  count = int(count) + 1\n",
        "\n",
        "# count = 1\n",
        "# for i in range(2,268,3):\n",
        "#   f = FPS(read_points(df[df.columns[i-2]],df[df.columns[i-1]],df[df.columns[i]]))\n",
        "#   C = f.compute_fps(100)#Number of sampling points\n",
        "#   file.writelines(\"marker\"+str(count)+\"\\n\")\n",
        "#   count = count + 1\n",
        "#   for j in C:\n",
        "#     file.writelines(str(float(j[0]))+\"\\t\"+str(float(j[1]))+\"\\t\"+str(float(j[2]))+\"\\n\")"
      ],
      "metadata": {
        "id": "0BMoYs791c0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebb6c40-c83a-4042-a93f-7204810a9c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "init start\n",
            "init finish\n",
            "finish output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read sampled data from csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "sampled_train_dir = '/content/drive/MyDrive/CapstoneDataset/sampled_training'\n",
        "sampled_test_dir = '/content/drive/MyDrive/CapstoneDataset/sampled_test'\n",
        "sampled_file_number = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n",
        "sampled_train_data = []\n",
        "sampled_test_data = []\n",
        "for number in sampled_file_number:\n",
        "  print(sampled_train_dir+'/'+number+'.csv')\n",
        "  df = pd.read_csv(sampled_train_dir+'/'+number+'.csv')\n",
        "  sampled_train_data.append(df)\n",
        "\n",
        "for number in sampled_file_number:\n",
        "  print(sampled_test_dir+'/'+number+'.csv')\n",
        "  df = pd.read_csv(sampled_test_dir+'/'+number+'.csv')\n",
        "  sampled_test_data.append(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XWnpuhllrjv",
        "outputId": "f6352bef-6051-4ab7-e643-5982e6c17d19"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/1.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/2.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/3.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/4.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/5.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/6.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/7.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/8.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/9.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/10.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/11.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/12.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/13.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/14.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/15.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/16.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/17.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/18.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/1.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/2.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/3.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/4.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/5.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/6.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/7.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/8.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/9.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/10.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/11.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/12.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/13.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/14.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/15.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/16.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/17.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/18.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "from itertools import chain\n",
        "\n",
        "# Compute the center point of 89*100 points.\n",
        "def compute_centroid(pc):\n",
        "  centroid = []\n",
        "  x = pd.DataFrame()\n",
        "  y = pd.DataFrame()\n",
        "  z = pd.DataFrame()\n",
        "  for i in range(0,265,3):\n",
        "    newX = pd.DataFrame(pc.iloc[:,i].values)\n",
        "    x = x.append(newX,ignore_index=True)\n",
        "  centroid.append(x.sum()/len(x))\n",
        "  for j in range(1,266,3):\n",
        "    newY = pd.DataFrame(pc.iloc[:,j].values)\n",
        "    y = y.append(newY,ignore_index=True)\n",
        "  centroid.append(y.sum()/len(y))\n",
        "  for k in range(2,267,3):\n",
        "    newZ = pd.DataFrame(pc.iloc[:,k].values)\n",
        "    z = z.append(newZ,ignore_index=True)\n",
        "  centroid.append(z.sum()/len(z))\n",
        "  centroid = pd.DataFrame(centroid)\n",
        "  return centroid\n",
        "\n",
        "# centroid = compute_centroid(sampled_train_data[0])\n",
        "\n",
        "def normalize_point_cloud(pc):\n",
        "    centroid = compute_centroid(pc) # compute center of point cloud\n",
        "    centroid = centroid.T\n",
        "    centroid = centroid.values\n",
        "    centroid = list(chain.from_iterable(centroid))\n",
        "    # put centre of the point cloud to (0, 0, 0)\n",
        "    for i in range(0,265,3):\n",
        "      pc.iloc[:,i] = pc.iloc[:,i] - centroid[0]\n",
        "    for j in range(1,266,3):\n",
        "      pc.iloc[:,j] = pc.iloc[:,j] - centroid[1]\n",
        "    for k in range(2,267,3):\n",
        "      pc.iloc[:,k] = pc.iloc[:,k] - centroid[2]\n",
        "    \n",
        "    # find the longest axis in XYZ axis and compute the length. This step can get a scaling ratio.\n",
        "    # Step 1: The point cloud after translation is squared\n",
        "    # Step 2: Sum according to row. This step can get a 100*1 matrix. The original matrix is 100*267\n",
        "    # Step 3: Find the square root and find the maxiumn value as scaling ratio\n",
        "    m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n",
        "\n",
        "    # Scaling point cloud according the ratio\n",
        "    pc_normalized = pc / m # normalize point cloud to (-1,1) according to long axis\n",
        "    return pc, pc_normalized, centroid, m  # centroid: center point, m: length of long axis, centroid and m can be used to compute keypoints\n",
        "\n",
        "\n",
        "# Export normalization data to csv\n",
        "normalize_train_dir = '/content/drive/MyDrive/CapstoneDataset/normalize_training'\n",
        "normalize_test_dir = '/content/drive/MyDrive/CapstoneDataset/normalize_test'\n",
        "\n",
        "count = 1\n",
        "for data in sampled_train_data:\n",
        "  print(count)\n",
        "  (pc, pc_normalized, centroid, length) = normalize_point_cloud(data)\n",
        "  if(pc_normalized.min().min() >= -1 and pc_normalized.max().max() <= 1):\n",
        "    pc_normalized.to_csv(normalize_train_dir+'/'+str(count)+'.csv', index=False)\n",
        "    print('finish output')\n",
        "    count = int(count) + 1\n",
        "\n",
        "count = 1\n",
        "for data in sampled_test_data:\n",
        "  print(count)\n",
        "  (pc, pc_normalized, centroid, length) = normalize_point_cloud(data)\n",
        "  if(pc_normalized.min().min() >= -1 and pc_normalized.max().max() <= 1):\n",
        "    pc_normalized.to_csv(normalize_test_dir+'/'+str(count)+'.csv', index=False)\n",
        "    print('finish output')\n",
        "    count = int(count) + 1"
      ],
      "metadata": {
        "id": "_X1oicFnyS0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453f66fc-160c-45aa-d23d-cb9fbfe661f3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "finish output\n",
            "2\n",
            "finish output\n",
            "3\n",
            "finish output\n",
            "4\n",
            "finish output\n",
            "5\n",
            "finish output\n",
            "6\n",
            "finish output\n",
            "7\n",
            "finish output\n",
            "8\n",
            "finish output\n",
            "9\n",
            "finish output\n",
            "10\n",
            "finish output\n",
            "11\n",
            "finish output\n",
            "12\n",
            "finish output\n",
            "13\n",
            "finish output\n",
            "14\n",
            "finish output\n",
            "15\n",
            "finish output\n",
            "16\n",
            "finish output\n",
            "17\n",
            "finish output\n",
            "18\n",
            "finish output\n",
            "1\n",
            "finish output\n",
            "2\n",
            "finish output\n",
            "3\n",
            "finish output\n",
            "4\n",
            "finish output\n",
            "5\n",
            "finish output\n",
            "6\n",
            "finish output\n",
            "7\n",
            "finish output\n",
            "8\n",
            "finish output\n",
            "9\n",
            "finish output\n",
            "10\n",
            "finish output\n",
            "11\n",
            "finish output\n",
            "12\n",
            "finish output\n",
            "13\n",
            "finish output\n",
            "14\n",
            "finish output\n",
            "15\n",
            "finish output\n",
            "16\n",
            "finish output\n",
            "17\n",
            "finish output\n",
            "18\n",
            "finish output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read normalization data from csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "normalize_file_number = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n",
        "normalize_train_data = []\n",
        "normalize_test_data = []\n",
        "for number in normalize_file_number:\n",
        "  print(normalize_train_dir+'/'+number+'.csv')\n",
        "  df = pd.read_csv(normalize_train_dir+'/'+number+'.csv')\n",
        "  normalize_train_data.append(df)\n",
        "\n",
        "for number in normalize_file_number:\n",
        "  print(normalize_test_dir+'/'+number+'.csv')\n",
        "  df = pd.read_csv(normalize_test_dir+'/'+number+'.csv')\n",
        "  normalize_test_data.append(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO4M_tdQXavb",
        "outputId": "25685a8d-4e7f-4ff8-e32c-9089af17c131"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/1.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/2.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/3.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/4.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/5.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/6.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/7.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/8.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/9.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/10.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/11.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/12.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/13.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/14.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/15.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/16.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/17.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/18.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/1.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/2.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/3.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/4.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/5.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/6.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/7.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/8.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/9.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/10.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/11.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/12.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/13.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/14.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/15.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/16.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/17.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/18.csv\n"
          ]
        }
      ]
    }
  ]
}