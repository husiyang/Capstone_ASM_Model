{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jxKyUPyKAg-",
        "outputId": "0afb0270-872f-4b85-9ac3-18f1d723033d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Read dataset from csv file**"
      ],
      "metadata": {
        "id": "jwN4elhPlnjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EisClCzUH5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307f6518-a957-4329-e8de-fc8a11b70738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CapstoneDataset/training/01_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/02_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/03_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/04_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/05_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/06_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/07_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/08_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/09_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/10_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/11_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/12_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/13_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/14_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/15_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/16_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/17_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/training/18_train.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/01_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/02_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/03_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/04_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/05_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/06_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/07_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/08_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/09_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/10_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/11_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/12_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/13_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/14_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/15_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/16_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/17_test.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/test/18_test.csv\n"
          ]
        }
      ],
      "source": [
        "# Read dataset from csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/CapstoneDataset/training'\n",
        "test_dir = '/content/drive/MyDrive/CapstoneDataset/test'\n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "file_number = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18']\n",
        "\n",
        "for number in file_number:\n",
        "  print(train_dir+'/'+number+'_train.csv')\n",
        "  df = pd.read_csv(train_dir+'/'+number+'_train.csv',header=5)\n",
        "  df = df.drop('Frame', axis=1)\n",
        "  df = df.drop('Time (Seconds)', axis=1)\n",
        "  train_data.append(df)\n",
        "\n",
        "for number in file_number:\n",
        "  print(test_dir+'/'+number+'_test.csv')\n",
        "  df = pd.read_csv(test_dir+'/'+number+'_test.csv',header=5)\n",
        "  df = df.drop('Frame', axis=1)\n",
        "  df = df.drop('Time (Seconds)', axis=1)\n",
        "  test_data.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Use Furthest Point Sampling and export data to csv**\n",
        "This part took me over one hour. I exported the final result to csv file. I won't run this part again."
      ],
      "metadata": {
        "id": "9h1hsHcRoQxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Furthest Point Sampling\n",
        "from os import remove\n",
        "def read_points(x,y,z):\n",
        "    points = np.stack([x, y, z], axis=1)\n",
        "    return points\n",
        "\n",
        "''' Strategy of FPS\n",
        "Step 1: Use the first point in the point cloud as the query point and take the furthest point from the remaining points\n",
        "Step 2: Continue with the point taken out as the query point and take the furthest point from the remaining points. \n",
        "Now, since the number of points already taken out is greater than 1, each point in the set of points already selected needs to be taken into account, \n",
        "The calculation logic is as follows.\n",
        "    Step 2.1: For any of the remaining points, calculate the distance from that point to all points in the selected point set.\n",
        "    Step 2.2: Take the minimum value, which is the distance from the remaining points to the selected set of points.\n",
        "    Step 2.3: After calculating the distance of each remaining point to the point set, take the point with the maxiumn distance.\n",
        "Step 3: repeat step 2 until the number K=100.\n",
        "'''\n",
        "class FPS:\n",
        "    # eliminate duplicate points\n",
        "    def __init__(self, points):\n",
        "        print('init start')\n",
        "        self.points = np.unique(points, axis=0)\n",
        "        print('init finish')\n",
        "\n",
        "    # calculate the distance from any of the remaining points to all points in the selected point set. np.stack()\n",
        "    # Take the minumum distance as the the distance from the remaining points to the selected set of points. np.min()\n",
        "    # After calculating the distance of each remaining point to the point set, take the point with the maxiumn distance. np.argmax()\n",
        "    def get_min_distance(self, a, b):\n",
        "        distance = []\n",
        "        for i in range(a.shape[0]):\n",
        "            dis = np.sum(np.square(a[i] - b), axis=-1)\n",
        "            distance.append(dis)\n",
        "        distance = np.stack(distance, axis=-1)\n",
        "        distance = np.min(distance, axis=-1)\n",
        "        return np.argmax(distance)\n",
        "    \n",
        "    # find minimun xyz value and maximum xyz value。\n",
        "    # Arrange and combine all values to obtain the cornermost point set in the point cloud.\n",
        "    @staticmethod\n",
        "    def get_model_corners(model):\n",
        "        min_x, max_x = np.min(model[:, 0]), np.max(model[:, 0])\n",
        "        min_y, max_y = np.min(model[:, 1]), np.max(model[:, 1])\n",
        "        min_z, max_z = np.min(model[:, 2]), np.max(model[:, 2])\n",
        "        corners_3d = np.array([\n",
        "            [min_x, min_y, min_z],\n",
        "            [min_x, min_y, max_z],\n",
        "            [min_x, max_y, min_z],\n",
        "            [min_x, max_y, max_z],\n",
        "            [max_x, min_y, min_z],\n",
        "            [max_x, min_y, max_z],\n",
        "            [max_x, max_y, min_z],\n",
        "            [max_x, max_y, max_z],\n",
        "        ])\n",
        "        return corners_3d\n",
        "    def compute_fps(self, K):\n",
        "        # compute centre point location according to the cornermost point set\n",
        "        corner_3d = self.get_model_corners(self.points)\n",
        "        center_3d = (np.max(corner_3d, 0) + np.min(corner_3d, 0)) / 2\n",
        "        A = np.array([center_3d])\n",
        "        B = np.array(self.points)\n",
        "        t = []\n",
        "        # looking for k nodes\n",
        "        for i in range(K):\n",
        "            max_id = self.get_min_distance(A, B)\n",
        "            A = np.append(A, np.array([B[max_id]]), 0)\n",
        "            B = np.delete(B, max_id, 0)\n",
        "            t.append(max_id)\n",
        "        return A, t\n",
        "\n",
        "# Calculate the center point of 89 markers. The result is a nrows*3 matrix.\n",
        "def compute_center_point(dataframe):\n",
        "  points = pd.DataFrame(data=None,columns=['X','Y','Z'])\n",
        "  index = 0\n",
        "  size = dataframe.shape[1]\n",
        "  for row in dataframe.index:\n",
        "    x = []\n",
        "    y = []\n",
        "    z = []\n",
        "    for i in range(2,size+1,3):\n",
        "      x.append(dataframe.loc[row][i-2])\n",
        "      y.append(dataframe.loc[row][i-1])\n",
        "      z.append(dataframe.loc[row][i])\n",
        "    points.loc[index] = [sum(x)/len(x),sum(y)/len(y),sum(z)/len(z)]\n",
        "    index = index + 1\n",
        "  return points\n",
        "\n",
        "# Export sampled data to csv\n",
        "sampled_train_dir = '/content/drive/MyDrive/CapstoneDataset/sampled_training'\n",
        "sampled_test_dir = '/content/drive/MyDrive/CapstoneDataset/sampled_test'\n",
        "\n",
        "\n",
        "# First step, calculate the center point of 89 markers in each frame.\n",
        "# Second step, use FPS to find the 100 furthest points and index.\n",
        "# Third step, back to the original dataset to find corresponding 89 markers. 89*100 matrix.\n",
        "count = 1\n",
        "for df in train_data:\n",
        "  points = compute_center_point(df)\n",
        "  print(count)\n",
        "  f = FPS(read_points(points['X'],points['Y'],points['Z']))\n",
        "\n",
        "  # Get the index of the 100 furthest points\n",
        "  # Change the sampling number here\n",
        "  (C,index) = f.compute_fps(100)\n",
        "\n",
        "  # create a new dataframe has the same column as old df\n",
        "  # return to original point cloud and get 89 points according to index. The result is a 100*89 matrix.\n",
        "  data1 = pd.DataFrame(columns = df.columns.to_list())\n",
        "  for i in index:\n",
        "    data1 = data1.append(df.loc[i],ignore_index=True)\n",
        "  data1.to_csv(sampled_train_dir+'/'+str(count)+'.csv', index=False)\n",
        "  print('finish output')\n",
        "  count = int(count) + 1\n",
        "\n",
        "count = 1\n",
        "for df in test_data:\n",
        "  points = compute_center_point(df)\n",
        "  print(count)\n",
        "  f = FPS(read_points(points['X'],points['Y'],points['Z']))\n",
        "\n",
        "  # Change the sampling number here\n",
        "  (C,index) = f.compute_fps(100)\n",
        "\n",
        "  # create a new dataframe has the same column as old df and export it to csv\n",
        "  data1 = pd.DataFrame(columns = df.columns.to_list())\n",
        "  for i in index:\n",
        "    data1 = data1.append(df.loc[i],ignore_index=True)\n",
        "  data1.to_csv(sampled_test_dir+'/'+str(count)+'.csv', index=False)\n",
        "  print('finish output')\n",
        "  count = int(count) + 1\n",
        "\n",
        "# count = 1\n",
        "# for i in range(2,268,3):\n",
        "#   f = FPS(read_points(df[df.columns[i-2]],df[df.columns[i-1]],df[df.columns[i]]))\n",
        "#   C = f.compute_fps(100)#Number of sampling points\n",
        "#   file.writelines(\"marker\"+str(count)+\"\\n\")\n",
        "#   count = count + 1\n",
        "#   for j in C:\n",
        "#     file.writelines(str(float(j[0]))+\"\\t\"+str(float(j[1]))+\"\\t\"+str(float(j[2]))+\"\\n\")"
      ],
      "metadata": {
        "id": "0BMoYs791c0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebb6c40-c83a-4042-a93f-7204810a9c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "init start\n",
            "init finish\n",
            "finish output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Read sampled data from csv**"
      ],
      "metadata": {
        "id": "aac7bjjamBjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read sampled data from csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "sampled_train_dir = '/content/drive/MyDrive/CapstoneDataset/sampled_training'\n",
        "sampled_test_dir = '/content/drive/MyDrive/CapstoneDataset/sampled_test'\n",
        "sampled_file_number = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n",
        "sampled_train_data = []\n",
        "sampled_test_data = []\n",
        "for number in sampled_file_number:\n",
        "  if number != '14':\n",
        "    print(sampled_train_dir+'/'+number+'.csv')\n",
        "    df = pd.read_csv(sampled_train_dir+'/'+number+'.csv')\n",
        "    sampled_train_data.append(df)\n",
        "\n",
        "for number in sampled_file_number:\n",
        "  if number != '14':\n",
        "    print(sampled_test_dir+'/'+number+'.csv')\n",
        "    df = pd.read_csv(sampled_test_dir+'/'+number+'.csv')\n",
        "    sampled_test_data.append(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XWnpuhllrjv",
        "outputId": "1df3c58f-9342-4cda-c84b-b2f0c4943d31"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/1.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/2.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/3.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/4.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/5.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/6.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/7.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/8.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/9.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/10.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/11.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/12.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/13.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/15.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/16.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/17.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_training/18.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/1.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/2.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/3.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/4.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/5.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/6.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/7.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/8.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/9.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/10.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/11.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/12.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/13.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/15.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/16.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/17.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/sampled_test/18.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Normalization**\n",
        "I use the point cloud normalization strategy. Export data to csv."
      ],
      "metadata": {
        "id": "vGcs4iN_mG6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_test_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "LhkaRspTJ_cy",
        "outputId": "9c5971b8-df28-43ed-97a6-ee035d9f0197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           X         Y         Z       X.1       Y.1       Z.1       X.2  \\\n",
              "0   0.434963  0.422864  0.116870  0.411616  0.397577  0.159184  0.379768   \n",
              "1   0.434507  0.432018  0.093004  0.413885  0.407089  0.136777  0.383881   \n",
              "2   0.493815  0.443950  0.154903  0.465639  0.413508  0.184705  0.438761   \n",
              "3   0.435764  0.517374  0.132633  0.422962  0.482907  0.166779  0.394214   \n",
              "4   0.492502  0.437814  0.149007  0.465475  0.409884  0.182304  0.437079   \n",
              "..       ...       ...       ...       ...       ...       ...       ...   \n",
              "95  0.436846  0.424890  0.118869  0.413165  0.399692  0.160895  0.381339   \n",
              "96  0.432949  0.438816  0.112555  0.413109  0.412364  0.155371  0.382742   \n",
              "97  0.438386  0.517686  0.130052  0.428966  0.482877  0.164566  0.399824   \n",
              "98  0.430501  0.427902  0.117854  0.407836  0.402124  0.159871  0.377291   \n",
              "99  0.437663  0.423059  0.118521  0.414555  0.397366  0.160636  0.382677   \n",
              "\n",
              "         Y.2       Z.2       X.3  ...      Z.85      X.86      Y.86      Z.86  \\\n",
              "0   0.393446  0.197402  0.372607  ...  0.277063  0.596774 -0.007099  0.208347   \n",
              "1   0.403446  0.176707  0.379308  ...  0.253253  0.595161  0.005245  0.185066   \n",
              "2   0.403676  0.220923  0.431270  ...  0.276280  0.617295 -0.020251  0.206213   \n",
              "3   0.473095  0.208975  0.386691  ...  0.279238  0.591671  0.035793  0.206894   \n",
              "4   0.402744  0.218669  0.430529  ...  0.276129  0.617894 -0.020168  0.206546   \n",
              "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
              "95  0.395719  0.199160  0.374085  ...  0.274901  0.597684 -0.006715  0.206616   \n",
              "96  0.407904  0.194889  0.379224  ...  0.253861  0.592682  0.006932  0.186160   \n",
              "97  0.474934  0.207663  0.390653  ...  0.274011  0.591589  0.029590  0.201508   \n",
              "98  0.397648  0.199049  0.370745  ...  0.275360  0.591589 -0.004274  0.205324   \n",
              "99  0.392712  0.198682  0.375614  ...  0.278203  0.597852 -0.007360  0.209931   \n",
              "\n",
              "        X.87      Y.87      Z.87      X.88      Y.88      Z.88  \n",
              "0   0.583374 -0.009286  0.156665  0.532232 -0.019313  0.096958  \n",
              "1   0.580828  0.001597  0.134623  0.528581 -0.007302  0.076955  \n",
              "2   0.603368 -0.020923  0.154475  0.550394 -0.026201  0.096102  \n",
              "3   0.574161  0.038644  0.157503  0.523476  0.040360  0.104495  \n",
              "4   0.603858 -0.020445  0.154959  0.551240 -0.023945  0.096700  \n",
              "..       ...       ...       ...       ...       ...       ...  \n",
              "95  0.583696 -0.008364  0.155496  0.532854 -0.015346  0.096443  \n",
              "96  0.577283  0.004213  0.136654  0.524823 -0.001618  0.079986  \n",
              "97  0.572973  0.032693  0.152909  0.522235  0.034887  0.100828  \n",
              "98  0.577089 -0.005444  0.153940  0.523741 -0.010480  0.096438  \n",
              "99  0.584975 -0.009616  0.158372  0.534769 -0.019123  0.098123  \n",
              "\n",
              "[100 rows x 267 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a36a6ce-580b-459f-a6aa-94a290c11c08\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>X.1</th>\n",
              "      <th>Y.1</th>\n",
              "      <th>Z.1</th>\n",
              "      <th>X.2</th>\n",
              "      <th>Y.2</th>\n",
              "      <th>Z.2</th>\n",
              "      <th>X.3</th>\n",
              "      <th>...</th>\n",
              "      <th>Z.85</th>\n",
              "      <th>X.86</th>\n",
              "      <th>Y.86</th>\n",
              "      <th>Z.86</th>\n",
              "      <th>X.87</th>\n",
              "      <th>Y.87</th>\n",
              "      <th>Z.87</th>\n",
              "      <th>X.88</th>\n",
              "      <th>Y.88</th>\n",
              "      <th>Z.88</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.434963</td>\n",
              "      <td>0.422864</td>\n",
              "      <td>0.116870</td>\n",
              "      <td>0.411616</td>\n",
              "      <td>0.397577</td>\n",
              "      <td>0.159184</td>\n",
              "      <td>0.379768</td>\n",
              "      <td>0.393446</td>\n",
              "      <td>0.197402</td>\n",
              "      <td>0.372607</td>\n",
              "      <td>...</td>\n",
              "      <td>0.277063</td>\n",
              "      <td>0.596774</td>\n",
              "      <td>-0.007099</td>\n",
              "      <td>0.208347</td>\n",
              "      <td>0.583374</td>\n",
              "      <td>-0.009286</td>\n",
              "      <td>0.156665</td>\n",
              "      <td>0.532232</td>\n",
              "      <td>-0.019313</td>\n",
              "      <td>0.096958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.434507</td>\n",
              "      <td>0.432018</td>\n",
              "      <td>0.093004</td>\n",
              "      <td>0.413885</td>\n",
              "      <td>0.407089</td>\n",
              "      <td>0.136777</td>\n",
              "      <td>0.383881</td>\n",
              "      <td>0.403446</td>\n",
              "      <td>0.176707</td>\n",
              "      <td>0.379308</td>\n",
              "      <td>...</td>\n",
              "      <td>0.253253</td>\n",
              "      <td>0.595161</td>\n",
              "      <td>0.005245</td>\n",
              "      <td>0.185066</td>\n",
              "      <td>0.580828</td>\n",
              "      <td>0.001597</td>\n",
              "      <td>0.134623</td>\n",
              "      <td>0.528581</td>\n",
              "      <td>-0.007302</td>\n",
              "      <td>0.076955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.493815</td>\n",
              "      <td>0.443950</td>\n",
              "      <td>0.154903</td>\n",
              "      <td>0.465639</td>\n",
              "      <td>0.413508</td>\n",
              "      <td>0.184705</td>\n",
              "      <td>0.438761</td>\n",
              "      <td>0.403676</td>\n",
              "      <td>0.220923</td>\n",
              "      <td>0.431270</td>\n",
              "      <td>...</td>\n",
              "      <td>0.276280</td>\n",
              "      <td>0.617295</td>\n",
              "      <td>-0.020251</td>\n",
              "      <td>0.206213</td>\n",
              "      <td>0.603368</td>\n",
              "      <td>-0.020923</td>\n",
              "      <td>0.154475</td>\n",
              "      <td>0.550394</td>\n",
              "      <td>-0.026201</td>\n",
              "      <td>0.096102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.435764</td>\n",
              "      <td>0.517374</td>\n",
              "      <td>0.132633</td>\n",
              "      <td>0.422962</td>\n",
              "      <td>0.482907</td>\n",
              "      <td>0.166779</td>\n",
              "      <td>0.394214</td>\n",
              "      <td>0.473095</td>\n",
              "      <td>0.208975</td>\n",
              "      <td>0.386691</td>\n",
              "      <td>...</td>\n",
              "      <td>0.279238</td>\n",
              "      <td>0.591671</td>\n",
              "      <td>0.035793</td>\n",
              "      <td>0.206894</td>\n",
              "      <td>0.574161</td>\n",
              "      <td>0.038644</td>\n",
              "      <td>0.157503</td>\n",
              "      <td>0.523476</td>\n",
              "      <td>0.040360</td>\n",
              "      <td>0.104495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.492502</td>\n",
              "      <td>0.437814</td>\n",
              "      <td>0.149007</td>\n",
              "      <td>0.465475</td>\n",
              "      <td>0.409884</td>\n",
              "      <td>0.182304</td>\n",
              "      <td>0.437079</td>\n",
              "      <td>0.402744</td>\n",
              "      <td>0.218669</td>\n",
              "      <td>0.430529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.276129</td>\n",
              "      <td>0.617894</td>\n",
              "      <td>-0.020168</td>\n",
              "      <td>0.206546</td>\n",
              "      <td>0.603858</td>\n",
              "      <td>-0.020445</td>\n",
              "      <td>0.154959</td>\n",
              "      <td>0.551240</td>\n",
              "      <td>-0.023945</td>\n",
              "      <td>0.096700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.436846</td>\n",
              "      <td>0.424890</td>\n",
              "      <td>0.118869</td>\n",
              "      <td>0.413165</td>\n",
              "      <td>0.399692</td>\n",
              "      <td>0.160895</td>\n",
              "      <td>0.381339</td>\n",
              "      <td>0.395719</td>\n",
              "      <td>0.199160</td>\n",
              "      <td>0.374085</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274901</td>\n",
              "      <td>0.597684</td>\n",
              "      <td>-0.006715</td>\n",
              "      <td>0.206616</td>\n",
              "      <td>0.583696</td>\n",
              "      <td>-0.008364</td>\n",
              "      <td>0.155496</td>\n",
              "      <td>0.532854</td>\n",
              "      <td>-0.015346</td>\n",
              "      <td>0.096443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.432949</td>\n",
              "      <td>0.438816</td>\n",
              "      <td>0.112555</td>\n",
              "      <td>0.413109</td>\n",
              "      <td>0.412364</td>\n",
              "      <td>0.155371</td>\n",
              "      <td>0.382742</td>\n",
              "      <td>0.407904</td>\n",
              "      <td>0.194889</td>\n",
              "      <td>0.379224</td>\n",
              "      <td>...</td>\n",
              "      <td>0.253861</td>\n",
              "      <td>0.592682</td>\n",
              "      <td>0.006932</td>\n",
              "      <td>0.186160</td>\n",
              "      <td>0.577283</td>\n",
              "      <td>0.004213</td>\n",
              "      <td>0.136654</td>\n",
              "      <td>0.524823</td>\n",
              "      <td>-0.001618</td>\n",
              "      <td>0.079986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.438386</td>\n",
              "      <td>0.517686</td>\n",
              "      <td>0.130052</td>\n",
              "      <td>0.428966</td>\n",
              "      <td>0.482877</td>\n",
              "      <td>0.164566</td>\n",
              "      <td>0.399824</td>\n",
              "      <td>0.474934</td>\n",
              "      <td>0.207663</td>\n",
              "      <td>0.390653</td>\n",
              "      <td>...</td>\n",
              "      <td>0.274011</td>\n",
              "      <td>0.591589</td>\n",
              "      <td>0.029590</td>\n",
              "      <td>0.201508</td>\n",
              "      <td>0.572973</td>\n",
              "      <td>0.032693</td>\n",
              "      <td>0.152909</td>\n",
              "      <td>0.522235</td>\n",
              "      <td>0.034887</td>\n",
              "      <td>0.100828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.430501</td>\n",
              "      <td>0.427902</td>\n",
              "      <td>0.117854</td>\n",
              "      <td>0.407836</td>\n",
              "      <td>0.402124</td>\n",
              "      <td>0.159871</td>\n",
              "      <td>0.377291</td>\n",
              "      <td>0.397648</td>\n",
              "      <td>0.199049</td>\n",
              "      <td>0.370745</td>\n",
              "      <td>...</td>\n",
              "      <td>0.275360</td>\n",
              "      <td>0.591589</td>\n",
              "      <td>-0.004274</td>\n",
              "      <td>0.205324</td>\n",
              "      <td>0.577089</td>\n",
              "      <td>-0.005444</td>\n",
              "      <td>0.153940</td>\n",
              "      <td>0.523741</td>\n",
              "      <td>-0.010480</td>\n",
              "      <td>0.096438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.437663</td>\n",
              "      <td>0.423059</td>\n",
              "      <td>0.118521</td>\n",
              "      <td>0.414555</td>\n",
              "      <td>0.397366</td>\n",
              "      <td>0.160636</td>\n",
              "      <td>0.382677</td>\n",
              "      <td>0.392712</td>\n",
              "      <td>0.198682</td>\n",
              "      <td>0.375614</td>\n",
              "      <td>...</td>\n",
              "      <td>0.278203</td>\n",
              "      <td>0.597852</td>\n",
              "      <td>-0.007360</td>\n",
              "      <td>0.209931</td>\n",
              "      <td>0.584975</td>\n",
              "      <td>-0.009616</td>\n",
              "      <td>0.158372</td>\n",
              "      <td>0.534769</td>\n",
              "      <td>-0.019123</td>\n",
              "      <td>0.098123</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 267 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a36a6ce-580b-459f-a6aa-94a290c11c08')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a36a6ce-580b-459f-a6aa-94a290c11c08 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a36a6ce-580b-459f-a6aa-94a290c11c08');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "from itertools import chain\n",
        "\n",
        "# Compute the center point of 89*100 points.\n",
        "def compute_centroid(pc):\n",
        "  centroid = []\n",
        "  x = pd.DataFrame()\n",
        "  y = pd.DataFrame()\n",
        "  z = pd.DataFrame()\n",
        "  for i in range(0,265,3):\n",
        "    newX = pd.DataFrame(pc.iloc[:,i].values)\n",
        "    x = x.append(newX,ignore_index=True)\n",
        "  centroid.append(x.sum()/len(x))\n",
        "  for j in range(1,266,3):\n",
        "    newY = pd.DataFrame(pc.iloc[:,j].values)\n",
        "    y = y.append(newY,ignore_index=True)\n",
        "  centroid.append(y.sum()/len(y))\n",
        "  for k in range(2,267,3):\n",
        "    newZ = pd.DataFrame(pc.iloc[:,k].values)\n",
        "    z = z.append(newZ,ignore_index=True)\n",
        "  centroid.append(z.sum()/len(z))\n",
        "  centroid = pd.DataFrame(centroid)\n",
        "  return centroid\n",
        "\n",
        "# centroid = compute_centroid(sampled_train_data[0])\n",
        "\n",
        "def normalize_point_cloud(pc):\n",
        "    centroid = compute_centroid(pc) # compute center of point cloud\n",
        "    centroid = centroid.T\n",
        "    centroid = centroid.values\n",
        "    centroid = list(chain.from_iterable(centroid))\n",
        "    # put centre of the point cloud to (0, 0, 0)\n",
        "    for i in range(0,265,3):\n",
        "      pc.iloc[:,i] = pc.iloc[:,i] - centroid[0]\n",
        "    for j in range(1,266,3):\n",
        "      pc.iloc[:,j] = pc.iloc[:,j] - centroid[1]\n",
        "    for k in range(2,267,3):\n",
        "      pc.iloc[:,k] = pc.iloc[:,k] - centroid[2]\n",
        "    \n",
        "    # find the longest axis in XYZ axis and compute the length. This step can get a scaling ratio.\n",
        "    # Step 1: The point cloud after translation is squared\n",
        "    # Step 2: Sum according to row. This step can get a 100*1 matrix. The original matrix is 100*267\n",
        "    # Step 3: Find the square root and find the maxiumn value as scaling ratio\n",
        "    m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n",
        "\n",
        "    # Scaling point cloud according the ratio\n",
        "    pc_normalized = pc / m # normalize point cloud to (-1,1) according to long axis\n",
        "    return pc, pc_normalized, centroid, m  # centroid: center point, m: length of long axis, centroid and m can be used to compute keypoints\n",
        "\n",
        "\n",
        "# Export normalization data to csv\n",
        "normalize_train_dir = '/content/drive/MyDrive/CapstoneDataset/normalize_training'\n",
        "normalize_test_dir = '/content/drive/MyDrive/CapstoneDataset/normalize_test'\n",
        "\n",
        "count = 1\n",
        "for data in sampled_train_data:\n",
        "  print(count)\n",
        "  (pc, pc_normalized, centroid, length) = normalize_point_cloud(data)\n",
        "  if(pc_normalized.min().min() >= -1 and pc_normalized.max().max() <= 1):\n",
        "    pc_normalized.to_csv(normalize_train_dir+'/'+str(count)+'.csv', index=False)\n",
        "    print('finish output')\n",
        "    count = int(count) + 1\n",
        "\n",
        "count = 1\n",
        "for data in sampled_test_data:\n",
        "  print(count)\n",
        "  (pc, pc_normalized, centroid, length) = normalize_point_cloud(data)\n",
        "  if(pc_normalized.min().min() >= -1 and pc_normalized.max().max() <= 1):\n",
        "    pc_normalized.to_csv(normalize_test_dir+'/'+str(count)+'.csv', index=False)\n",
        "    print('finish output')\n",
        "    count = int(count) + 1"
      ],
      "metadata": {
        "id": "_X1oicFnyS0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453f66fc-160c-45aa-d23d-cb9fbfe661f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "finish output\n",
            "2\n",
            "finish output\n",
            "3\n",
            "finish output\n",
            "4\n",
            "finish output\n",
            "5\n",
            "finish output\n",
            "6\n",
            "finish output\n",
            "7\n",
            "finish output\n",
            "8\n",
            "finish output\n",
            "9\n",
            "finish output\n",
            "10\n",
            "finish output\n",
            "11\n",
            "finish output\n",
            "12\n",
            "finish output\n",
            "13\n",
            "finish output\n",
            "14\n",
            "finish output\n",
            "15\n",
            "finish output\n",
            "16\n",
            "finish output\n",
            "17\n",
            "finish output\n",
            "18\n",
            "finish output\n",
            "1\n",
            "finish output\n",
            "2\n",
            "finish output\n",
            "3\n",
            "finish output\n",
            "4\n",
            "finish output\n",
            "5\n",
            "finish output\n",
            "6\n",
            "finish output\n",
            "7\n",
            "finish output\n",
            "8\n",
            "finish output\n",
            "9\n",
            "finish output\n",
            "10\n",
            "finish output\n",
            "11\n",
            "finish output\n",
            "12\n",
            "finish output\n",
            "13\n",
            "finish output\n",
            "14\n",
            "finish output\n",
            "15\n",
            "finish output\n",
            "16\n",
            "finish output\n",
            "17\n",
            "finish output\n",
            "18\n",
            "finish output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Read normalization data from csv**"
      ],
      "metadata": {
        "id": "YssMVmEbmci5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read normalization data from csv\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "normalize_train_dir = '/content/drive/MyDrive/CapstoneDataset/normalize_training'\n",
        "normalize_test_dir = '/content/drive/MyDrive/CapstoneDataset/normalize_test'\n",
        "normalize_file_number = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18']\n",
        "normalize_train_data = []\n",
        "normalize_test_data = []\n",
        "for number in normalize_file_number:\n",
        "  print(normalize_train_dir+'/'+number+'.csv')\n",
        "  df = pd.read_csv(normalize_train_dir+'/'+number+'.csv')\n",
        "  normalize_train_data.append(df)\n",
        "\n",
        "for number in normalize_file_number:\n",
        "  print(normalize_test_dir+'/'+number+'.csv')\n",
        "  df = pd.read_csv(normalize_test_dir+'/'+number+'.csv')\n",
        "  normalize_test_data.append(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO4M_tdQXavb",
        "outputId": "3c7d9249-f7cf-49a4-ff97-b78ddb7cd547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/1.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/2.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/3.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/4.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/5.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/6.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/7.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/8.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/9.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/10.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/11.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/12.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/13.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/14.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/15.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/16.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/17.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_training/18.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/1.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/2.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/3.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/4.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/5.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/6.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/7.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/8.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/9.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/10.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/11.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/12.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/13.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/14.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/15.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/16.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/17.csv\n",
            "/content/drive/MyDrive/CapstoneDataset/normalize_test/18.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BP Neural Network**\n",
        "https://blog.csdn.net/zhazhaqiangdedad/article/details/124920479?spm=1001.2014.3001.5501\n",
        "\n",
        "https://blog.csdn.net/weixin_41477928/article/details/123337949"
      ],
      "metadata": {
        "id": "PpztdqehMXqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split #training and test set split function\n",
        "import torch\n",
        "import torch.nn.functional as Fun\n",
        "\n",
        "lr        = 0.00001 # learning rate\n",
        "epochs    = 60000  # training round number \n",
        "n_feature = 3    # input node (x,y,z)\n",
        "n_hidden  = 20   # hidden layer\n",
        "n_output  = 89   # output node (89 classes)\n",
        "\n",
        "\n",
        "# def data_gen(data):\n",
        "#   newData = np.zeros((89,100,3))\n",
        "#   index = 0\n",
        "#   for i in range(0,265,3):\n",
        "#     newData[index] = data.iloc[:,[i,i+1,i+2]].values\n",
        "#     index = index + 1    \n",
        "#   return newData\n",
        "\n",
        "# def label_gen():\n",
        "#   combined_labels = np.zeros((89,100,89))\n",
        "#   for i in range(89):\n",
        "#     for j in range(100):\n",
        "#       combined_labels[i][j][i]=1\n",
        "#   return combined_labels\n",
        "\n",
        "# sampled_train_data[13] and sampled_test_data[13] has problem, so abondon it.\n",
        "# put all data into 3 columns, 100*17*89=151300. This is training set\n",
        "def data_gen(data):\n",
        "  newData = np.zeros((151300,3))\n",
        "  index = 0\n",
        "  for i in range(0,265,3):\n",
        "    for j in range(1700):\n",
        "      newData[index][0] = data.iloc[j,i]\n",
        "      newData[index][1] = data.iloc[j,i+1]\n",
        "      newData[index][2] = data.iloc[j,i+2]\n",
        "      index = index + 1   \n",
        "  return newData\n",
        "\n",
        "# put sampled_train_data[0] data into 3 columns. This is test set.\n",
        "def data_gen2(data):\n",
        "  newData = np.zeros((8900,3))\n",
        "  index = 0\n",
        "  for i in range(0,265,3):\n",
        "    for j in range(100):\n",
        "      newData[index][0] = data.iloc[j,i]\n",
        "      newData[index][1] = data.iloc[j,i+1]\n",
        "      newData[index][2] = data.iloc[j,i+2]\n",
        "      index = index + 1   \n",
        "  return newData\n",
        "\n",
        "# create labels for all data. This is labels for training set\n",
        "def label_gen():\n",
        "  # labels = np.zeros(151300)\n",
        "  combined_labels = np.array([0]*1700)\n",
        "  for i in range(1,89):\n",
        "    combined_labels = np.append(combined_labels,[i]*1700)\n",
        "  # for i in range(len(labels)):\n",
        "  #   labels[i][combined_labels[i]] = 1\n",
        "  return combined_labels\n",
        "\n",
        "# create labels for sampled_test_data[0] data. This is labels for test set\n",
        "def label_gen2():\n",
        "  # labels = np.zeros(8900)\n",
        "  combined_labels = np.array([0]*100)\n",
        "  for i in range(1,89):\n",
        "    combined_labels = np.append(combined_labels,[i]*100)\n",
        "  # for i in range(len(labels)):\n",
        "  #   labels[i][combined_labels[i]] = 1\n",
        "  return combined_labels\n",
        "\n",
        "df_train = sampled_train_data[0]\n",
        "for i in range(1,17):\n",
        "  df_train = pd.concat([df_train,sampled_train_data[i]],axis=0)\n",
        "x_train = data_gen(df_train)\n",
        "y_train = label_gen()\n",
        "x_test = data_gen2(sampled_test_data[0])\n",
        "y_test = label_gen2()\n",
        "\n",
        "# x_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.2,random_state=22)\n",
        "\n",
        "# normalization here\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_train = min_max_scaler.fit_transform(x_train)\n",
        "x_test = min_max_scaler.fit_transform(x_test)\n",
        "\n",
        "# change data to tensor\n",
        "x_train=torch.FloatTensor(x_train)\n",
        "y_train=torch.LongTensor(y_train)\n",
        "x_test=torch.FloatTensor(x_test)\n",
        "y_test=torch.LongTensor(y_test)\n",
        "\n",
        "#2.create BP Neural Network\n",
        "class BPNetModel(torch.nn.Module):\n",
        "    def __init__(self,n_feature,n_hidden,n_output):\n",
        "        super(BPNetModel, self).__init__()\n",
        "        self.hiddden= torch.nn.Linear(n_feature,n_hidden)#define hidden layer\n",
        "        self.out=torch.nn.Linear(n_hidden,n_output)#define output layer\n",
        "    def forward(self,x):\n",
        "        x=Fun.relu(self.hiddden(x)) #hidden layer activation function use relu() function\n",
        "        out=Fun.softmax(self.out(x),dim=1) #output layer use softmax() function\n",
        "        return out\n",
        "#3.Define the optimizer and loss function\n",
        "net=BPNetModel(n_feature=n_feature,n_hidden=n_hidden,n_output=n_output) #Call the network\n",
        "optimizer=torch.optim.Adam(net.parameters(),lr=lr,eps=1e-3) #Use the Adam Optimizer and set the learning rate\n",
        "loss_fun=torch.nn.CrossEntropyLoss() #The cross entropy loss function is generally used for multi-classification\n",
        "\n",
        "#4.train data\n",
        "loss_steps=np.zeros(epochs) #an array([ 0., 0., 0., 0., 0.]) contains 0, number is epochs\n",
        "accuracy_steps=np.zeros(epochs)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    y_pred=net(x_train) #Forward propagation\n",
        "    loss=loss_fun(y_pred,y_train)#Comparison of predicted and real values\n",
        "    optimizer.zero_grad() #Gradient reset to 0\n",
        "    loss.backward() #Back propagation\n",
        "    optimizer.step() #Update gradient\n",
        "    loss_steps[epoch]=loss.item()# store loss\n",
        "    running_loss = loss.item()\n",
        "    # print(f\"{epoch} training，loss={running_loss}\".format(epoch,running_loss))\n",
        "    with torch.no_grad(): #Below is the calculation of no gradients, mainly used by the test set, no need to calculate gradients anymore\n",
        "        y_test_pred=net(x_test)\n",
        "        correct=(torch.argmax(y_test_pred)==y_test).type(torch.FloatTensor)\n",
        "        accuracy_steps[epoch]=correct.mean()\n",
        "    # print(\"accuracy\", accuracy_steps[epoch])\n",
        "\n",
        "#5.Plot loss function and precision\n",
        "fig_name=\"dataset_classify_BPNet\"\n",
        "fontsize=15\n",
        "fig,(ax1,ax2)=plt.subplots(2,figsize=(15,12),sharex=False)\n",
        "ax1.plot(accuracy_steps)\n",
        "ax1.set_ylabel(\"test accuracy\",fontsize=fontsize)\n",
        "ax1.set_title(fig_name,fontsize=\"xx-large\")\n",
        "ax2.plot(loss_steps)\n",
        "ax2.set_ylabel(\"train lss\",fontsize=fontsize)\n",
        "ax2.set_xlabel(\"epochs\",fontsize=fontsize)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u4GkISymBAQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = np.zeros(8900)\n",
        "index = 0\n",
        "for pred in y_test_pred:\n",
        "  res[index] = np.argmax(pred)\n",
        "  index = index + 1\n",
        "res\n",
        "  # pred = np.zeros(8900)\n",
        "  # for i in range(8900):\n",
        "  #   pred[i] = np.argmax(y_test_pred[i])\n",
        "  # pred_label = np.zeros((8900,89))\n",
        "  # index = 0\n",
        "  # for label in pred:\n",
        "  #   pred_label[index][int(label)] = 1\n",
        "  #   index = index + 1"
      ],
      "metadata": {
        "id": "jxlgbnQ91c94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971f411c-af40-4ba7-a9e5-bb4aaab43911"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([37., 37., 37., ..., 37., 37., 37.])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Xgboost**"
      ],
      "metadata": {
        "id": "2qrdfuqUr2uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# These functions are same as above\n",
        "def data_gen(data):\n",
        "  newData = np.zeros((151300,3))\n",
        "  index = 0\n",
        "  for i in range(0,265,3):\n",
        "    for j in range(1700):\n",
        "      newData[index][0] = data.iloc[j,i]\n",
        "      newData[index][1] = data.iloc[j,i+1]\n",
        "      newData[index][2] = data.iloc[j,i+2]\n",
        "      index = index + 1   \n",
        "  return newData\n",
        "\n",
        "def data_gen2(data):\n",
        "  newData = np.zeros((8900,3))\n",
        "  index = 0\n",
        "  for i in range(0,265,3):\n",
        "    for j in range(100):\n",
        "      newData[index][0] = data.iloc[j,i]\n",
        "      newData[index][1] = data.iloc[j,i+1]\n",
        "      newData[index][2] = data.iloc[j,i+2]\n",
        "      index = index + 1   \n",
        "  return newData\n",
        "\n",
        "def label_gen():\n",
        "  # labels = np.zeros(151300)\n",
        "  combined_labels = np.array([0]*1700)\n",
        "  for i in range(1,89):\n",
        "    combined_labels = np.append(combined_labels,[i]*1700)\n",
        "  # for i in range(len(labels)):\n",
        "  #   labels[i][combined_labels[i]] = 1\n",
        "  return combined_labels\n",
        "\n",
        "def label_gen2():\n",
        "  # labels = np.zeros(8900)\n",
        "  combined_labels = np.array([0]*100)\n",
        "  for i in range(1,89):\n",
        "    combined_labels = np.append(combined_labels,[i]*100)\n",
        "  # for i in range(len(labels)):\n",
        "  #   labels[i][combined_labels[i]] = 1\n",
        "  return combined_labels\n",
        "\n",
        "df_train = sampled_train_data[0]\n",
        "for i in range(1,17):\n",
        "  df_train = pd.concat([df_train,sampled_train_data[i]],axis=0)\n",
        "x_train = data_gen(df_train)\n",
        "y_train = label_gen()\n",
        "x_test = data_gen2(sampled_test_data[0])\n",
        "y_test = label_gen2()\n",
        "# X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.25, random_state=100)\n",
        "\n",
        "xgb_train=xgb.DMatrix(x_train,label=y_train)\n",
        "xgb_test=xgb.DMatrix(x_test,label=y_test)\n",
        "\n",
        "\n",
        "\n",
        "# Set the model parameters\n",
        "\n",
        "params={\n",
        "    'eta': 0.00001,\n",
        "    'max_depth': 12,\n",
        "    'subsample': 0.5,\n",
        "    'objective': 'multi:softmax',\n",
        "    'nthread': -1,\n",
        "    'silent': 1,\n",
        "    'booster': 'gbtree',\n",
        "    'num_class': 89\n",
        "}\n",
        "\n",
        "watchlist=[(xgb_train,'train'),(xgb_test,'test')]\n",
        "# Set up training rounds. 1000 rounds here\n",
        "num_round=100\n",
        "bst=xgb.train(params,\n",
        "              xgb_train,\n",
        "              num_round,\n",
        "              watchlist,\n",
        "              verbose_eval=10, # display 10 batch per time\n",
        "              early_stopping_rounds=100)\n",
        "\n",
        "# prediction\n",
        "\n",
        "pred=bst.predict(xgb_test)\n",
        "# print(pred)\n",
        "\n",
        "# evaluation\n",
        "\n",
        "# error_rate=np.sum(pred!=test.lable)/test.lable.shape[0]\n",
        "error_rate=np.sum(pred!=y_test)/y_test.shape[0]\n",
        "\n",
        "# This is the error rate of test set\n",
        "print('测试集错误率(softmax):{}'.format(error_rate))\n",
        "\n",
        "# this is the accuracy of the test set\n",
        "accuray=1-error_rate\n",
        "print('测试集准确率：%.4f' %accuray)"
      ],
      "metadata": {
        "id": "foHsp1Jdr6yu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9775a9e-b2e9-4dc0-f93b-b65d954c5333"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-merror:0.223239\ttest-merror:0.403371\n",
            "Multiple eval metrics have been passed: 'test-merror' will be used for early stopping.\n",
            "\n",
            "Will train until test-merror hasn't improved in 100 rounds.\n",
            "[10]\ttrain-merror:0.172022\ttest-merror:0.316517\n",
            "[20]\ttrain-merror:0.169736\ttest-merror:0.309888\n",
            "[30]\ttrain-merror:0.16959\ttest-merror:0.310112\n",
            "[40]\ttrain-merror:0.167964\ttest-merror:0.308427\n",
            "[50]\ttrain-merror:0.168215\ttest-merror:0.311348\n",
            "[60]\ttrain-merror:0.16764\ttest-merror:0.313371\n",
            "[70]\ttrain-merror:0.167118\ttest-merror:0.31382\n",
            "[80]\ttrain-merror:0.167488\ttest-merror:0.31382\n",
            "[90]\ttrain-merror:0.167462\ttest-merror:0.311798\n",
            "[99]\ttrain-merror:0.166332\ttest-merror:0.310562\n",
            "测试集错误率(softmax):0.310561797752809\n",
            "测试集准确率：0.6894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Convolutional Neural Network(unfinished)**"
      ],
      "metadata": {
        "id": "gjEN-azMiWs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import paddle as paddle\n",
        "import paddle.fluid as fluid\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def convolutional_neural_network(img):\n",
        "    # 第一个卷积-池化层\n",
        "    conv_pool_1 = fluid.nets.simple_img_conv_pool(\n",
        "        input=img,         # 输入图像\n",
        "        filter_size=5,     # 滤波器的大小\n",
        "        num_filters=20,    # filter 的数量。它与输出的通道相同\n",
        "        pool_size=2,       # 池化核大小2*2\n",
        "        pool_stride=2,     # 池化步长\n",
        "        act=\"relu\")        # 激活类型\n",
        "    conv_pool_1 = fluid.layers.batch_norm(conv_pool_1)\n",
        "    # 第二个卷积-池化层\n",
        "    conv_pool_2 = fluid.nets.simple_img_conv_pool(\n",
        "        input=conv_pool_1,\n",
        "        filter_size=5,\n",
        "        num_filters=50,\n",
        "        pool_size=2,\n",
        "        pool_stride=2,\n",
        "        act=\"relu\")\n",
        "    conv_pool_2 = fluid.layers.batch_norm(conv_pool_2)\n",
        "    # 第三个卷积-池化层\n",
        "    conv_pool_3 = fluid.nets.simple_img_conv_pool(\n",
        "        input=conv_pool_2,\n",
        "        filter_size=5,\n",
        "        num_filters=50,\n",
        "        pool_size=2,\n",
        "        pool_stride=2,\n",
        "        act=\"relu\")\n",
        "    # 以softmax为激活函数的全连接输出层，10类数据输出10个数字\n",
        "    prediction = fluid.layers.fc(input=conv_pool_3, size=89, act='softmax')\n",
        "    return prediction\n",
        "\n",
        "data_shape = [3, 32, 32]\n",
        "images = fluid.layers.data(name='images', shape=data_shape, dtype='float32')\n",
        "label = fluid.layers.data(name='label', shape=[1], dtype='int64')\n",
        "\n",
        "predict =  convolutional_neural_network(images)\n",
        "\n",
        "# 获取损失函数和准确率\n",
        "cost = fluid.layers.cross_entropy(input=predict, label=label) # 交叉熵\n",
        "avg_cost = fluid.layers.mean(cost)                            # 计算cost中所有元素的平均值\n",
        "acc = fluid.layers.accuracy(input=predict, label=label)       #使用输入和标签计算准确率\n",
        "\n",
        "# 获取测试程序\n",
        "test_program = fluid.default_main_program().clone(for_test=True)\n",
        "\n",
        "# 定义优化方法\n",
        "optimizer =fluid.optimizer.Adam(learning_rate=0.001)\n",
        "optimizer.minimize(avg_cost)\n",
        "print(\"完成\")\n",
        "\n",
        "all_train_iter=0\n",
        "all_train_iters=[]\n",
        "all_train_costs=[]\n",
        "all_train_accs=[]\n",
        "def draw_train_process(title,iters,costs,accs,label_cost,lable_acc):\n",
        "    plt.title(title, fontsize=24)\n",
        "    plt.xlabel(\"iter\", fontsize=20)\n",
        "    plt.ylabel(\"cost/acc\", fontsize=20)\n",
        "    plt.plot(iters, costs,color='red',label=label_cost) \n",
        "    plt.plot(iters, accs,color='green',label=lable_acc) \n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "EPOCH_NUM = 20\n",
        "model_save_dir = \"/home/aistudio/work/catdog.inference.model\"\n",
        "\n",
        "for pass_id in range(EPOCH_NUM):\n",
        "    # 开始训练\n",
        "    for batch_id, data in enumerate(train_reader()):                        #遍历train_reader的迭代器，并为数据加上索引batch_id\n",
        "        train_cost,train_acc = exe.run(program=fluid.default_main_program(),#运行主程序\n",
        "                             feed=feeder.feed(data),                        #喂入一个batch的数据\n",
        "                             fetch_list=[avg_cost, acc])                    #fetch均方误差和准确率\n",
        "\n",
        "        \n",
        "        all_train_iter=all_train_iter+BATCH_SIZE\n",
        "        all_train_iters.append(all_train_iter)\n",
        "        all_train_costs.append(train_cost[0])\n",
        "        all_train_accs.append(train_acc[0])\n",
        "        \n",
        "        #每100次batch打印一次训练、进行一次测试\n",
        "        if batch_id % 100 == 0:                                             \n",
        "            print('Pass:%d, Batch:%d, Cost:%0.5f, Accuracy:%0.5f' % \n",
        "            (pass_id, batch_id, train_cost[0], train_acc[0]))\n",
        "            \n",
        "\n",
        "    # 开始测试\n",
        "    test_costs = []                                                         #测试的损失值\n",
        "    test_accs = []                                                          #测试的准确率\n",
        "    for batch_id, data in enumerate(test_reader()):\n",
        "        test_cost, test_acc = exe.run(program=test_program,                 #执行测试程序\n",
        "                                      feed=feeder.feed(data),               #喂入数据\n",
        "                                      fetch_list=[avg_cost, acc])           #fetch 误差、准确率\n",
        "        test_costs.append(test_cost[0])                                     #记录每个batch的误差\n",
        "        test_accs.append(test_acc[0])                                       #记录每个batch的准确率\n",
        "    \n",
        "    # 求测试结果的平均值\n",
        "    test_cost = (sum(test_costs) / len(test_costs))                         #计算误差平均值（误差和/误差的个数）\n",
        "    test_acc = (sum(test_accs) / len(test_accs))                            #计算准确率平均值（ 准确率的和/准确率的个数）\n",
        "    print('Test:%d, Cost:%0.5f, ACC:%0.5f' % (pass_id, test_cost, test_acc))\n",
        "    \n",
        "#保存模型\n",
        "# 如果保存路径不存在就创建\n",
        "if not os.path.exists(model_save_dir):\n",
        "    os.makedirs(model_save_dir)\n",
        "print ('save models to %s' % (model_save_dir))\n",
        "fluid.io.save_inference_model(model_save_dir,\n",
        "                              ['images'],\n",
        "                              [predict],\n",
        "                              exe)\n",
        "print('训练模型保存完成！')\n",
        "draw_train_process(\"training\",all_train_iters,all_train_costs,all_train_accs,\"trainning cost\",\"trainning acc\")"
      ],
      "metadata": {
        "id": "FN3gyoQtia6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}